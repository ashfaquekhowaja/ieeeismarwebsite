{"id":833,"date":"2024-09-26T04:12:50","date_gmt":"2024-09-26T04:12:50","guid":{"rendered":"https:\/\/ieeeismar.org\/?page_id=833"},"modified":"2024-10-16T18:12:11","modified_gmt":"2024-10-16T18:12:11","slug":"demos","status":"publish","type":"page","link":"https:\/\/ieeeismar.org\/demos\/","title":{"rendered":"Demos"},"content":{"rendered":"\n<p><\/p>\n\n\n\n<h5 class=\"gutenakitid-6960d9c9-fb04-4ebe-9d7d-7f950a0dd9c0  wp-block-heading\">DEMO1115: Demonstration of FIRE: Mid-Air Thermo-Tactile Display<\/h5>\n\n\n\n<p><strong>Authors<\/strong><br>Yatharth Singhal, Haokun Wang, Jin Ryong Kim<\/p>\n\n\n\n<p><strong>Abstract<\/strong><br>We demonstrate an ultrasound haptic-based mid-air thermo-tactile display system. We design a proof-of-concept thermo-tactile feedback system with an open-top chamber, heat modules, and an ultrasound haptic display. Our method involves directing heated airflow toward the focused pressure point produced by the ultrasound display to deliver thermal and tactile cues in mid-air simultaneously. We demonstrate our system with three different VR environments (campfire, kitchen, and candle) to show the rich user experiences of integrating thermal and tactile feedback.<\/p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"><\/div>\n\n\n\n<h5 class=\"gutenakitid-88e42c08-f056-4c01-a9b9-4e11da6fe6b0  wp-block-heading\">DEMO1109: PORI: POrtal Recall Interface for Relationship Memories<\/h5>\n\n\n\n<p><strong>Authors<\/strong><br>Nahyeon Kim, Chungnyeong Lee, Jusub Kim<br><br><strong>Abstract<\/strong><br>We introduce a new AR+AI powered mobile App for capturing and reliving personal relationship memories. Photo and video have limitations as a medium for recording relationship memories. They don&#8217;t have the ability to record interactions with the subject of the relationship memory. It\u2019s also very difficult to provide an immersive re-experience through normal photo and video because the subject exists in a different visual environment than the user replaying the memory. We developed a next-generation personal memory recording and playback app that will allow people to relive their everyday interpersonal experiences more vividly than photos and videos.<\/p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"><\/div>\n\n\n\n<h5 class=\"gutenakitid-8b7b880d-5aeb-464e-9108-969740cf0eaa  wp-block-heading\">DEMO1086: Demonstrating XDTK: Prototyping Multi-Device Interaction and Arbitration in XR<\/h5>\n\n\n\n<p><strong>Authors<\/strong><br>Eric J Gonzalez, Ishan Chatterjee, Khushman Patel, Mar Gonzalez-Franco, Andrea Cola\u00e7o, Karan Ahuja<br><br><strong>Abstract<\/strong><br>The interaction space of XR head-mounted devices can be extended by leveraging other digital devices, such as phones, tablets, and smartwatches. We present a demonstration of XDTK (Cross-Device Toolkit), an open-sourced prototyping toolkit for multi-device interactions in XR. The toolkit consists of two parts: (1) an Android app that runs on client devices and surfaces pose, touch, and other sensor data to a (2) Unity server that can be added to any Unity-based XR application. For this demo, we specifically apply XDTK toward a few example applications, including multi-device arbitration. By leveraging pose data from each device, we can infer which device the user is gazing at to seamlessly hand off control and display between multiple devices. We also show examples leveraging a tablet sketching and a smartwatch for menu navigation.<\/p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"><\/div>\n\n\n\n<h5 class=\"wp-block-heading has-accent-color has-text-color has-link-color wp-elements-e9e61c052d44d3648aa1a5e2c753d289\">DEMO1083: Accented Character Entry Using Physical Keyboards in Virtual Reality<\/h5>\n\n\n\n<p><strong>Authors<\/strong><br>Snehanjali Kalamkar, Verena Biener, Daniel Pauls, Leon Lindlein, Morteza Izadifar, Per Ola Kristensson, Jens Grubert<\/p>\n\n\n\n<p><strong>Abstract<\/strong><br>In recent years, research on text entry in Virtual Reality (VR) has gained popularity but the efficient entry of accented characters (i.e. characters with diacritical marks) in VR remains underexplored. Entering accented characters is supported on most soft keyboards through a long press on a base character and subsequent selection of the accented character. However, entering those characters on physical keyboards is still challenging, as they require the recall and entry of respective numeric codes. To address this issue, within this work, we investigate three techniques to support accented character entry on physical keyboards in VR. Specifically, we compare a context-aware numeric code technique not requiring users to recall a code, a key-press-only condition in which the accented characters are dynamically remapped to physical keys next to a base character, and a multimodal technique, in which eye gaze is used to select the accented version of a base character previously selected by key-press on the keyboard. The results from our user study (n=18) reveal that both the key-press-only and the multimodal techniques outperform the baseline technique in terms of text entry speed.<\/p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"><\/div>\n\n\n\n<h5 class=\"wp-block-heading has-accent-color has-text-color has-link-color wp-elements-d601e367991cb6ed694c8652a7661ddc\">DEMO1122: Enhance Flight Experience Through Wind-based Cross-modal Effect<\/h5>\n\n\n\n<p><strong>Authors<\/strong><br>Yuan Li, Jiayi Hu, Du Jin, Juro Hosoi, Rui ZHANG, Yuki Ban, Shinichi Warisawa<\/p>\n\n\n\n<p><strong>Abstract<\/strong><br>Realizing an immersive flight experience in the field of virtual reality is of great interest in both academia and industry. Existing methods tend to achieve this by providing an intuitive sense of motion or physically suspending the user. However, these methods often place a significant demand on the user&#8217;s physical space or impose various restrictions on user interactions. In this study, we propose a new method that is expected to enhance the flight experience by achieving the sensation of floating without suspending the user. To do this, we developed a wind-based device that can give airflow stimulation to the soles of the user&#8217;s feet. A preliminary user study has been conducted, laying the foundation for our future works.<\/p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"><\/div>\n\n\n\n<h5 class=\"wp-block-heading has-accent-color has-text-color has-link-color wp-elements-9116e1ec00dead48c04fbe120723ee91\">DEMO1094: Visibility Modulation of Aligned Spaces for Multi-User Telepresence<\/h5>\n\n\n\n<p><strong>Authors<\/strong><br>Taehei Kim, Jihun Shin, Hyeshim Kim, Hyuckjin Jang, Jiho Kang, Sung-Hee Lee<\/p>\n\n\n\n<p><strong>Abstract<\/strong><br>We propose a multi-user Mixed Reality(MR) telepresence system that dynamically modulates the visibility of multiple users&#8217; rooms based on their locations. Our method optimizes room alignment to maximize a shared space, where users interact with common real or virtual objects and each other. We also visualize each user&#8217;s private, non-shared space to convey activities beyond the shared area. To address room overlap, we dynamically adjust private space visibility based on users\u2019 locations. Our system allows users to experience a seamlessly connected and shared multi-room environment. We also demonstrate a game that leverages the dynamic visibility modulation feature of our system.<\/p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"><\/div>\n\n\n\n<h5 class=\"wp-block-heading has-accent-color has-text-color has-link-color wp-elements-a4b2a83abc5dc51a459c5da9170ce46b\">DEMO1106: Virtual Reality for Immersive Education in Orthopedic Surgery Digital Twins<\/h5>\n\n\n\n<p><strong>Authors<\/strong><br>Jonas Hein, Jan David Grunder, Lilian Calvet, Fr\u00e9d\u00e9ric Giraud, Nicola Alessandro Cavalcanti, Fabio Carrillo, Philipp F\u00fcrnstahl<br><br><strong>Abstract<\/strong><br>TBA<\/p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"><\/div>\n\n\n\n<h5 class=\"wp-block-heading has-accent-color has-text-color has-link-color wp-elements-f33ba10a910fb15b7b48a23e1a0fe710\">DEMO1128: Virtual Dairy Farm: An Interactive Experience for Public Education<\/h5>\n\n\n\n<p><strong>Authors<\/strong><br>Anh Nguyen, Hyeongil Nam, Emma Windfeld, Michael Francis, Guillaume Lhermie, Kangsoo Kim<br><br><strong>Abstract<\/strong><br>Despite growing public interest in the origins and production methods of dairy products, driven by concerns about environmental impact, local sourcing, and ethics, a knowledge-trust gap persists between consumers and the dairy industry. To tackle this, our demo presents an immersive virtual farm simulation designed to provide realistic on-farm experiences to the public. Users can visit the virtual farm, explore various sites where dairy cows are raised, and learn about dairy production processes through this virtual experience. The simulation demonstrates significant potential as an effective tool for agricultural education.<\/p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"><\/div>\n\n\n\n<h5 class=\"wp-block-heading has-accent-color has-text-color has-link-color wp-elements-7774bca7d458ce117e526e36e5444a21\">DEMO1123: MetaGadget: IoT Framework for Event-Triggered Integration of User-Developed Devices into Commercial Metaverse Platforms<\/h5>\n\n\n\n<p><strong>Authors<\/strong><br>Ryutaro Kurai, Yuichi Hiroi, Takefumi Hiraki<br><br><strong>Abstract<\/strong><br>This demonstration introduces MetaGadget, an IoT framework designed to integrate user-developed devices into commercial metaverse platforms. Synchronizing virtual reality (VR) environments with physical devices has traditionally required a constant connection to VR clients, limiting flexibility and resource efficiency. MetaGadget overcomes these limitations by configuring user-developed devices as IoT units with server capabilities, supporting communication via HTTP protocols within the commercial metaverse platform, Cluster. This approach enables event-triggered device control without the need for persistent connections from metaverse clients. Through the demonstration, users will experience event-triggered interaction between VR and physical devices, as well as real-world device control through the VR space by multiple people. Our framework is expected to reduce technical barriers to integrating VR spaces and custom devices, contribute to interoperability, and increase resource efficiency through event-triggered connections.<\/p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"><\/div>\n\n\n\n<h5 class=\"wp-block-heading has-accent-color has-text-color has-link-color wp-elements-089c65db984b7ce40c27d2f38ca70253\">DEMO1124: Mitigating Latency Effects on Subjective Experience in Robot Teleoperation Using a VR-Enabled Virtual Spring<\/h5>\n\n\n\n<p><strong>Authors<\/strong><br>Du Jin, Rui ZHANG, Yuan Li, Yuki Ban, Shinichi Warisawa<br><br><strong>Abstract<\/strong><br>Remote robot teleoperation is crucial for tasks that require human oversight, yet latency can significantly impair operator performance and result in discomfort, break in presence and increased workload. In this paper, we propose a new robot teleoperation technique based on Virtual Reality that is expected to alleviate the negative impacts on subjective experience caused by latency. The technique allows users to manipulate a virtual robot synchronized with a real one by &#8216;grabbing&#8217; a virtual spring attached to it. Controller vibration is also used to simulate spring force feedback, together creating an illusion that the position discrepancy is caused by the dynamics of the spring instead of latency. We hypothesize that this approach can mitigate the negative effects of latency by making the robot&#8217;s movement appear less strange and more natural. A user study was conducted to evaluate the effectiveness of virtual spring and controller vibration separately using a 2&#215;2 factorial design. The results suggest that the virtual spring enhanced user comfort level and the sense of presence, but the controller vibration showed no clear benefits.<\/p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"><\/div>\n\n\n\n<h5 class=\"wp-block-heading has-accent-color has-text-color has-link-color wp-elements-5ad8a961df93017ed125cc4768a5a9a1\">DEMO1132: AI-Powered Mixed Reality: Revolutionizing Training Methodologies<\/h5>\n\n\n\n<p><strong>Authors<\/strong><br>Bence Bihari, B\u00e1lint Gy\u00f6rgy Nagy, J\u00e1nos D\u00f3ka, Balazs Sonkoly<br><br><strong>Abstract<\/strong><br>Training is a costly and time-consuming task, especially in industrial environments. Self-learning methods, customized curriculum, mixed reality (MR), and automation in the training process can have great societal impact. However, the preparation of new educational materials will pose additional challenges which could hinder the technology adaptation. In this demo, we propose a novel platform exploiting various AI technologies to facilitate and accelerate the creation of training applications, especially in MR environments. An expert needs to perform and explain a complex process only once and our system automatically generates a step-by-step tutorial including video snippets and textual descriptions.<\/p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"><\/div>\n\n\n\n<h5 class=\"wp-block-heading has-accent-color has-text-color has-link-color wp-elements-f709144741b1cfe05f9638369f51b659\">DEMO1120: Audyssey: Immersive Visualization of Popular Music Evolution<\/h5>\n\n\n\n<p><strong>Authors<\/strong><br>Dasol Lee, Jaesuk Lee, Semi Kwon, Jusub Kim<\/p>\n\n\n\n<p><strong>Abstract<\/strong><br>In this demo, we present a method to visualize the changes in pop music trends from 1958 to 2024 using Billboard Hot 100 chart data. We collected audio sources from YouTube and classified the music data by decade, from the 1960s to the 2010s, using a CNN model. The classified music data was then projected into a 3D space using UMAP and implemented as a VR experience. This allows users to visually and audibly experience the similarities and changes in music trends over time. This demo offers a new way to intuitively understand the temporal evolution of popular music.<\/p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"><\/div>\n\n\n\n<h5 class=\"wp-block-heading has-accent-color has-text-color has-link-color wp-elements-9242429515a1e72670aa7c6e33308c66\">DEMO1130: VHard: An XR UI for Kinesthetic Rehearsal of Rock Climbing Moves<\/h5>\n\n\n\n<p><strong>Authors<\/strong><br>Joel Kevles Salzman, Jace Li, Benjamin Yang, Steven Feiner<br><br><strong>Abstract<\/strong><br>Rock climbers frequently struggle to complete climbs due to the difficulty of practicing precise movements without actually attempting them. A standard strategy is to kinesthetically rehearse the hand and body positions required while on the ground. However, the lack of live feedback severely hampers this method because climbers can only roughly estimate the distances and positions involved. We introduce VHard, an extended reality (XR) application that visualizes the precision and accuracy of hand placements on a metric digital twin of a MoonBoard climbing wall. VHard augments kinesthetic rehearsal with XR feedback in order to assist climbers in completing difficult boulder problems.<\/p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"><\/div>\n\n\n\n<h5 class=\"wp-block-heading has-accent-color has-text-color has-link-color wp-elements-c60d40123e32059f4e3035d14a4fa09f\">DEMO1104: An XR GUI for Visualizing Messages in ECS Architectures<\/h5>\n\n\n\n<p><strong>Authors<\/strong><br>Benjamin Yang, Xichen He, Jace Li, Carmine Elvezio, Steven Feiner<\/p>\n\n\n\n<p><strong>Abstract<\/strong><br>Entity-Component-System (ECS) architectures are fundamental to many systems for developing extended reality (XR) applications. These applications often contain complex scenes and require intricately connected application logic to connect components together, making debugging and analysis difficult. Graph-based tools have been created to show actions in ECS-based scene hierarchies, but few address interactions that go beyond traditional hierarchical communication. To address this, we present an XR GUI for Mercury (a toolkit to handle cross-component ECS communication) that allows developers to view and edit relationships and interactions between scene entities in Mercury.<\/p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"><\/div>\n\n\n\n<h5 class=\"wp-block-heading has-accent-color has-text-color has-link-color wp-elements-28d58e24016da33c92845a237acff9b6\">DEMO1112: Demonstration of Fiery Hands: Thermal Gloves through Thermal and Tactile Integration<\/h5>\n\n\n\n<p><strong>Authors<\/strong><br>Haokun Wang, Yatharth Singhal, Hyunjae Gil, Jin Ryong Kim<\/p>\n\n\n\n<p><strong>Abstract<\/strong><br>We demonstrate a novel wearable thermal interface, designed and fabricated as a glove consisting of flexible thermoelectric devices (Peltier) and vibrotactile ERM motors. Our approach induced the thermal sensation through thermal and tactile integration by strategically placing the thermal actuator at the back side of the finger, presenting thermal feedback at the location of tactile actuators on the palmer side, and providing free hand manipulation. We showcase our system with two different VR environments: water faucet and magic fireball to illustrate the unique user experience of interacting with virtual objects.<\/p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"><\/div>\n\n\n\n<h5 class=\"wp-block-heading has-accent-color has-text-color has-link-color wp-elements-cd4b92f409451220fe23ee3dc7f7ce68\">DEMO1113: Demonstration of Thermal Flow Illusions with Tactile and Thermal Interaction<\/h5>\n\n\n\n<p><strong>Authors<\/strong><br>Yatharth Singhal, Daniel Honrales, Haokun Wang, Jin Ryong Kim<\/p>\n\n\n\n<p><strong>Abstract<\/strong><br>We present a novel interaction technique called thermal motion, which creates an illusion of flowing thermal sensations by combining thermal and tactile actuators. This technique generates dynamic thermal referral illusions across multiple tactile points, making users perceive moving thermal cues. Our demonstration uses a sleeve form factor to illustrate this effect, effectively creating the illusion of moving thermal cues along users&#8217; arms.<\/p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"><\/div>\n\n\n\n<h5 class=\"wp-block-heading has-accent-color has-text-color has-link-color wp-elements-ed6a5a3e7817ff406e587cbfab03eff5\">DEMO1114: Demonstration of Tangible Data: Immersive Data Exploration With Touch<\/h5>\n\n\n\n<p><strong>Authors<\/strong><br>Ayush Bhardwaj, Jin Ryong Kim<\/p>\n\n\n\n<p><strong>Abstract<\/strong><br>We demonstrate an interactive 3D data visualization tool called TangibleData. TangibleData adapts hand gestures and mid-air haptics to provide 3D data exploration and interaction in VR using hand gestures and ultrasound mid-air haptic feedback. We showcase different types of 3D visualization datasets with different data encoding methods that convert data into visual and haptic representations for data interaction. We focus on an intuitive approach for each dataset, using mid-air haptics to improve the user&#8217;s understanding. We transformed each dataset&#8217;s inherent properties (velocity, vorticity, density, volume, and location) into haptics to demonstrate a tangible experience for the user.<\/p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"><\/div>\n\n\n\n<h5 class=\"wp-block-heading has-accent-color has-text-color has-link-color wp-elements-f2534999ca98414424a1a47dba5541da\">DEMO1095: VR Simulation for Evaluating Pharmaceutical Delivery Assistance Systems from Sales Offices to Client Facilities<\/h5>\n\n\n\n<p><strong>Authors<\/strong><br>Shigeki Ozawa, Takumi Miyoshi, Ryosuke Ichikari, Takuya Miura, Takeshi Kurata<\/p>\n\n\n\n<p><strong>Abstract<\/strong><br>In delivery processes from pharmaceutical wholesaler&#8217;s sales offices to the client&#8217;s facilities, an assistance system using handheld devices and paper slips is used. The introduction of AR assistance systems is beginning to be investigated to improve operations in these processes. However, as AR is not yet highly accepted in Japanese industry, a VR environment has been constructed to enable the cycle consisting of prototyping, comparison with existing systems, and sharing the results as efficiently as possible. In this demo, the participants will be able to have a try of each assistance method in the VR environment.<\/p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"><\/div>\n\n\n\n<h5 class=\"wp-block-heading has-accent-color has-text-color has-link-color wp-elements-acdadc7aaa1421c634c229a517551d29\">DEMO1127: Lee Jungseop XR: A XR Documentary for Apple Vision Pro<\/h5>\n\n\n\n<p><strong>Authors<\/strong><br>Dasom Kim, Melodie Jannau, Yongsoon Choi, Sangyong Kim, Jusub Kim<\/p>\n\n\n\n<p><strong>Abstract<\/strong><br>Artworks and artifacts are stored in collections in museums or art storage facilities. Visible storage became popular in museums all over the world about 2010, allowing visitors to have a wider visual view of the exhibits and boosting accessibility and utilization. However, the appreciation of individual pieces may be hampered by this high-density display methods, and curators generally find it difficult to design exhibitions. By getting over physical constraints, eXtended Reality (XR) technology offers a workable solution that makes it easier to plan a variety of creative and unique exhibitions. This work suggests that a new category of XR documentary content be developed in response to the advent of Apple&#8217;s Vision Pro. The goal of the research is to determine whether XR technology can improve the experience in the visible storage exhibitions, which might have a significant impact on how collections are seen and used in the future.<\/p>\n\n\n\n<div style=\"height:30px\" aria-hidden=\"true\" class=\"wp-block-spacer\"><\/div>\n\n\n\n<h5 class=\"wp-block-heading has-accent-color has-text-color has-link-color wp-elements-745d8ef7bf6f62c96da344a0429669cc\">DEMO1118: Visual Guidance for Assembly Processes<\/h5>\n\n\n\n<p><strong>Authors<\/strong><br>Julian Kreimeier, Hannah Schieber, Shiyu Li, Alejandro Martin-Gomez, Daniel Roth<\/p>\n\n\n\n<p><strong>Abstract<\/strong><br>TBA<\/p>\n\n\n\n<div style=\"height:50px\" aria-hidden=\"true\" class=\"wp-block-spacer\"><\/div>\n\n\n\n<p><\/p>\n","protected":false},"excerpt":{"rendered":"<p>DEMO1115: Demonstration of FIRE: Mid-Air Thermo-Tactile Display AuthorsYatharth Singhal, Haokun Wang, Jin Ryong Kim AbstractWe demonstrate an ultrasound haptic-based mid-air thermo-tactile display system. We design a proof-of-concept thermo-tactile feedback system with an open-top chamber, heat modules, and an ultrasound haptic display. Our method involves directing heated airflow toward the focused [&hellip;]<\/p>\n","protected":false},"author":1,"featured_media":0,"parent":0,"menu_order":0,"comment_status":"closed","ping_status":"closed","template":"page-templates\/template-fullwidth.php","meta":{"footnotes":""},"class_list":["post-833","page","type-page","status-publish","hentry"],"_links":{"self":[{"href":"https:\/\/ieeeismar.org\/wp-json\/wp\/v2\/pages\/833","targetHints":{"allow":["GET"]}}],"collection":[{"href":"https:\/\/ieeeismar.org\/wp-json\/wp\/v2\/pages"}],"about":[{"href":"https:\/\/ieeeismar.org\/wp-json\/wp\/v2\/types\/page"}],"author":[{"embeddable":true,"href":"https:\/\/ieeeismar.org\/wp-json\/wp\/v2\/users\/1"}],"replies":[{"embeddable":true,"href":"https:\/\/ieeeismar.org\/wp-json\/wp\/v2\/comments?post=833"}],"version-history":[{"count":16,"href":"https:\/\/ieeeismar.org\/wp-json\/wp\/v2\/pages\/833\/revisions"}],"predecessor-version":[{"id":961,"href":"https:\/\/ieeeismar.org\/wp-json\/wp\/v2\/pages\/833\/revisions\/961"}],"wp:attachment":[{"href":"https:\/\/ieeeismar.org\/wp-json\/wp\/v2\/media?parent=833"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}